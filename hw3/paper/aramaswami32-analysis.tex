\documentclass[11pt]{article}
    \usepackage{amsmath}
    \usepackage[pdftex]{graphicx}
    %\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
    \usepackage{amsmath,amsthm,amssymb,color,wrapfig,floatflt,amsfonts,listings,fancyhdr}
    \usepackage[parfill]{parskip}
    %\pagestyle{plain}
    \setlength\topmargin{.1cm}
    %\setlength\textheight{8in}
    \setlength\oddsidemargin{.1in}
    \setlength\evensidemargin{.1in}
    \setlength\textwidth{6.51in}
    %\setlength\headheight{52.5pt}
    \usepackage{fancyhdr}
    \usepackage[algosection, boxruled, linesnumbered]{algorithm2e}
    \pagestyle{fancy}
    \lhead{Arvind Ramaswami}
    \rhead{HW3}
    \chead{CS 4641 Machine Learning}
    \lfoot{}
    \cfoot{Page {\thepage}}
    \rfoot{}
    \newcommand{\RR}{\mathbb{R}}
    \newcommand{\CC}{\mathbb{C}}
    \newcommand{\HH}{\mathbb{H}}
    \newcommand{\BB}{\mathbb{B}}
    \newcommand{\ZZ}{\mathbb{Z}}
    \newcommand{\QQ}{\mathbb{Q}}
    \newcommand{\NN}{\mathbb{N}}
    \newcommand{\cP}{\mathcal{P}}
    \renewcommand{\And}{\wedge}
    \newcommand{\Or}{\vee}
    \newcommand{\Implies}{\Rightarrow}
    \newcommand{\Not}{\sim}
    \begin{document}
        \section{Introduction}
            
            This project investigates algorithms in unsupervised
            learning: PCA (Principal Component Analysis), K-means
            clustering and GMMs (Gaussian Mixture Models).

            \subsection{Experiments}
            
            First, two datasets are chosen. In this experiment, the datasets are the Mushrooms
            dataset from Kaggle and the Pima Indians Diabetes dataset from Kaggle.

            Then, two clustering algorithms (K-means clustering and GMMs via expectation maximization)
            are run.

            Then, the datasets are reduced into fewer dimensions using Principal Component Analysis.
            The clustering algorithms are re-performed on the reduced dataset.

            Then, the original results of the PCA algorithm on the Pima Indians
            Diabetes dataset are taken, and the neural network is trained on this reduced
            dataset. To evaluate the accuracy of the neural network, both the training
            and testing datasets are projected with the dimensionality function that had been generated
            before.


            Finally, the training dataset of the Pima Indians
            Diabetes dataset is clustered using the k-Means and GMM algorithms
            and reduced to fewer dimensions using the two algorithms. In these specific experiements, the k-Means
            algorithm reduces the data to the cluster-distance space (sklearn's \texttt{transform}
            function), while the GMM reduces each datapoint to the probabilities
            of the datapoint being in each cluster (sklearn's \texttt{predict\char`_proba} function).
            Then, with the clustering algorithms used on the training set,
            the test set is also reduced to those same features. A neural network
            is then trained on the training datasets produced by the two clustering
            algorithms.
           
        \section{Results and Analysis}

            \subsection{Initial clustering}


%                 Given enough data points, it is possible to make quantitative measurements to
%                 extrapolate how much information the clustering algorithm can give about the
%                 structure of the true labels.

%                 For each cluster number, if the number of occurrences of that number for data points
%                 for every label is about the same, then that cluster does not give much information about
%                 what separates the data. However, if a cluster prefers one class label over the other class
%                 labels, it does give a great insight about the structure of the labels.

%                 There are some slight patterns that can be seen by running the clustering algorithms
%                 on the raw datasets (but these patterns do not give much insight on how the
%                 labels themselves are distributed).

%                 Here are the results of the cluster categorizations that were produced when running either algorithm
%                 for this dataset (the cluster categories of the first 200 elements of the training set
%                 are shown, with a parameter such that the number of clusters is equal to 10):

                
%                 [8 7 0 1 7 7 4 3 7 1 4 6 6 7 4 4 3 7 6 0 0 6 9 3 7 8 7 6 1 9 3 1 7 7 1 4 6
%  3 7 0 9 6 1 6 7 1 1 0 6 4 7 1 0 6 6 1 1 7 0 9 9 7 0 7 6 7 3 7 1 9 4 1 4 0
%  6 7 7 5 5 4 1 6 6 6 0 1 0 0 6 3 9 7 1 0 6 0 5 1 4 7 3 4 4 7 5 1 6 7 6 4 6
%  7 7 6 0 1 1 9 4 6 6 0 6 4 1 7 4 1 4 1 3 3 1 6 7 7 7 4 7 7 7 6 7 7 3 0 7 1
%  8 9 0 4 7 7 3 6 6 1 9 0 0 5 4 6 7 1 4 6 0 0 6 7 9 9 0 4 7 3 9 4 0 4 1 6 6
%  6 9 7 3 1 3 6 6 8 1 6 7 7 4 3] (K-means)

%                 [5 1 1 3 1 1 1 3 1 6 1 3 3 1 1 1 6 1 6 1 1 3 4 0 1 5 1 3 0 4 3 6 1 1 6 1 3
%  3 1 1 4 3 3 3 1 6 6 1 3 8 1 6 1 3 3 6 6 1 3 4 4 1 1 1 3 1 3 1 3 4 1 3 1 6
%  6 1 1 2 2 1 6 3 3 3 3 3 1 1 6 6 4 1 3 1 6 1 2 0 8 1 4 1 1 1 2 6 3 1 6 1 3
%  1 1 3 1 6 0 4 1 6 3 1 3 8 6 1 1 6 1 6 3 0 3 6 1 1 1 1 1 1 1 6 1 1 3 1 1 6
%  3 4 1 1 1 1 3 3 3 6 4 1 1 4 1 6 1 6 1 6 9 1 3 1 4 4 3 1 1 3 4 1 1 1 3 3 3
%  3 4 1 6 3 0 3 3 5 6 3 1 1 6 0] (GMM)

%                 As a reference, here are the first 200 training labels:

%                 [0 0 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0
%   1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0
%   0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0
%   0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0
%   0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 0
%   0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0
%   0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0
%   1 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0
%   0 1 1 0 0 1 1 1]

%                 When comparing the results of K-means and GMMs side by side, we can see
%                 a consistency in their observation of the structure of the data. For example,
%                 the second and third elements are given the same predictions by both algorithms.
%                 Also, the last two examples are given the same predictions by both algorithms. These
%                 are just to name a few of the many examples.

%                 However, the clustering algorithms do not seem to learn what sets the labels apart from
%                 each other (or if they do, it's not seen very clearly by the outputs of the algorithms).

            For each of the two datasets,
            the GMM and K-means clustering were both run on the training set. To determine the optimal number
            of cluster, some form of a metric was plotted over the number of clusters. For K-means, this metric
            was WCSS (within-cluster sums of squares), while metric for GMMs was AIC(Alkaline
            Information Criterion) and BIC(Bayesian information criterion).
            Here are the plots generated:

            \subsubsection{Initial clustering: Pima Indians Diabetes dataset}

            \includegraphics[width=8cm]{../pima/clustering1/gmm_init.png}
            \includegraphics[width=8cm]{../pima/clustering1/km_init.png}

            In this dataset, a clear "elbow" can be seen at two clusters for the GMM,
            and there is a slight "elbow" at three clusters for the K-means.

            The gmm graph has an unintuitive dip at k = 6. However, this could
            have just been due to randomness; there is no striking behavior seen
            in the final experiment in the plot for the neural network when k = 6.
            
            % This problem
            % is a binary classification problem (predicts whether someone has diabetes or not),
            % so it is reasonable for the number of clusters to be close to two.

            \subsubsection{Initial clustering: Mushrooms dataset}

            \includegraphics[width=8cm]{../mushrooms/clustering1/gmm_init.png}
            \includegraphics[width=8cm]{../mushrooms/clustering1/km_init.png}

            These two graphs look quite different. The plot for Gaussian Mixture
            Models has a much sharper elbow, implying that this model does much
            better when given a higher number of clusters compared to a low number,
            unlike k-Means, which would run in the least cost manner when running
            it with a smaller number of components.

            A reasonable explanation for this disparity is that
            K-means is sensitive to outliers and would put them in a separate
            cluster by [3] and thus would need more clusters to minimize its cost.


            
            \subsection{PCA}

                PCA was run on a couple of the datasets. To make the datasets easy to visualize, the
                PCA was run such that the data was to be split into two components. Here are the
                results of the PCA that was performed:

            \subsubsection{PCA: Pima Indians Diabetes dataset}

            \includegraphics[width=8cm]{../pima/pca/diabetes_pca.png}

            When considering the visualization of the data reduced to two dimensions,
            it can clearly be seen that there are two different regions, the vertical
            region in the left hand side and the larger region in the right hand side.

            The regions in this data do not align with their labels; for each region,
            one half consists of cases of diabetes, and the other half consists of cases
            without diabetes.

            \subsubsection{PCA: Mushrooms dataset}

            \includegraphics[width=8cm]{../mushrooms/pca/mushrooms_pca.png}

            In this two-dimensional projection of the Mushrooms dataset, we can see clear candidates for clusters.
            There are two blue regions. Also, there is an overlap of blue and red in the bottom left corner. In addition
            to those main components, there are some outlying red and blue distributed around the upper right portion
            of the graph.

            \subsection{Clustering after PCA}

            The two datasets were clustered with the two algorithms
            after the PCA dimensionality reduction algorithms were run
            on them.

            \subsubsection{Clustering after PCA: Pima Indians Diabetes dataset}

            \includegraphics[width=8cm]{../pima/clustering2/gmm_pca.png}
            \includegraphics[width=8cm]{../pima/clustering2/km_pca.png}

            This plot still has a slight dip at k = 6, but it is much smaller
            than the dip that was created before PCA. Still, there is not any clear
            information that can be obtained from this dip.

            Aside from the dip, the graphs seem similar to the ones created before
            the PCA algorithm had been run (an elbow at 2 clusters for the GMM
            and at 3 clusters for the PCA).

            Here are images of the clustering algorithms on the 2-dimensional data.

            \includegraphics[width=8cm]{../pima/clustering2/cluster_gmm_pca.png}
            \includegraphics[width=8cm]{../pima/clustering2/cluster_km_pca.png}

            \subsubsection{Clustering after PCA: Mushrooms dataset}
            
            \includegraphics[width=8cm]{../mushrooms/clustering2/gmm_pca.png}
            \includegraphics[width=8cm]{../mushrooms/clustering2/km_pca.png}

            The above two graphs seem quite similar to each other. This is unlike how
            they were before PCA, where the GMM had an elbow at a much larger number
            of clusters compared to the GMM.

            Here are images of the clustering algorithms on the 2-dimensional data.

            \includegraphics[width=8cm]{../mushrooms/clustering2/cluster_gmm_pca.png}
            \includegraphics[width=8cm]{../mushrooms/clustering2/cluster_km_pca.png}

            \subsection{Training a neural network after PCA}

            The Pima Indians dataset was split such that 60 percent of it
            was training data and 20 percent was testing data.
            
            Cross-validation was not necessary, since the objective here is not
            to produce the best neural network possible, but to rather observe
            and analyze the similarities in the neural network performance
            based on how the dimensions are reduced. Cross-validation adds an
            overcomplication which detracts from the main purpose of these
            experiments.
            
            Here is a plot of the accuracy of the neural network (training and testing) with
            respect to the number of principal components generated in the
            PCA algorithm:

            \includegraphics[width=8cm]{../pima/nn_pca/nn_pca.png}

            \subsection{Clustering for dimensionality reduction}

            The Pima Indians diabetes was split into a training and testing set (60
            percent training and 20 percent testing).

            Then, the GMM and the k-Means algorithm were each used to reduce the dimensions of
            the data for the Pima Indian Diabetes dataset. For each number of clusters,
            a neural network was trained to produce a mapping from the reduced data
            to labels. Here are the results for the training and testing
            accuracies:

            \includegraphics[width=8cm]{../pima/clustering3/gmm_acc.png}
            \includegraphics[width=8cm]{../pima/clustering3/km_acc.png}



        \section{Sources}

        [1] https://www.kaggle.com/uciml/pima-indians-diabetes-database

        [2] https://www.kaggle.com/uciml/mushroom-classification

        [3] https://people.eecs.berkeley.edu/~jordan/courses/294-fall09/lectures/clustering/slides.pdf



    \end{document}